# Understanding Kubernetes internals

The Kubernetes Control Plane
 The (worker) nodes

COMPONENTS OF THE CONTROL PLANE
The etcd distributed persistent storage
 The API server
 The Scheduler
 The Controller Manager

COMPONENTS RUNNING ON THE WORKER NODES
The Kubelet
 The Kubernetes Service Proxy (kube-proxy)
 The Container Runtime (Docker, rkt, or others)

ADD-ON COMPONENTS
 The Kubernetes DNS server
 The Dashboard
 An Ingress controller
 Heapster, which we’ll talk about in chapter 14
 The Container Network Interface network plugin (we’ll explain it later in this
chapter)


kubectl get po -o custom-columns=POD:metadata.name,NODE:spec.nodeName --sort-by spec.nodeName -n kube-system


11.1.2 How Kubernetes uses etcd

Store the state of the cluster

The only component that talks to etcd directly is the Kubernetes API server.
$ etcdctl ls /registry

WHY THE NUMBER OF ETCD INSTANCES SHOULD BE AN ODD NUMBER

Usually, for large clusters, an etcd cluster of five or seven nodes is sufficient. It can
handle a two- or a three-node failure, respectively, which suffices in almost all situations.

What the API server does

crud
validation
authentication
http interface

AUTHENTICATION PLUGINS
Http header
Authorization: Bearer tokens

AUTHORIZATION PLUGINS

VALIDATING AND/OR MODIFYING THE RESOURCE IN THE REQUEST WITH ADMISSION CONTROL PLUGINS

 https://kubernetes.io/docs/admin/admission-controllers/.

After letting the request pass through all the Admission Control plugins, the API server
then validates the object, stores it in etcd, and returns a response to the client.

A Control Plane component can request to be notified when a resource is created, modified, or deleted.


kubectl get pods --watch



11.1.5 Understanding the Scheduler

assign pods to nodes
update pod data in the API setting the node
kubelet is notified and runs the pod

DEFAULT SCHEDULING ALGORITHM

Scheduler can either be config-
ured to suit your specific needs or infrastructure specifics, or it can even be replaced
with a custom implementation altogether. You could also run a Kubernetes cluster
without a Scheduler, but then you’d have to perform the scheduling manually.


Introducing the controllers running in the Controller Manager
Replication Manager (a controller for ReplicationController resources)
 ReplicaSet, DaemonSet, and Job controllers322
HAPTER 11 Understanding Kubernetes internals
 Deployment controller
 StatefulSet controller
 Node controller
 Service controller
 Endpoints controller
 Namespace controller
 PersistentVolume controller
 Others

controllers’ source code

https://github.com/kubernetes/
kubernetes/blob/master/pkg/controller


What the Kubelet does

Kubelet and the Service Proxy both run on the worker
nodes, where the actual pods containers run

register node with the API server
run container with driver
monitor running containers
container liveness probes
terminates containers

role of the Kubernetes Service Proxy

setup connection to API
load balance requests to multiple API servers


11.1.9 Introducing Kubernetes add-ons

HOW THE DNS SERVER WORKS
updates /etc/resolv.conf on every container

INGRESS CONTROLLERS
diferent implementations 
reverse Proxy
talks directly to pod


kubectl get events --watch

11.3 Understanding what a running pod is
pause container
Pod infrastructure container
Holds all the state of the pod 


Inter-pod networking
The network is set up by the system administrator or
by a Container Network Interface (CNI)

Introducing the Container Network Interface
https://kubernetes.io/docs/concepts/cluster-administration/addons/.

How services are implemented

Everything related to Services is handled by the kube-proxy process
kube-proxy uses iptables

service creation notifies all kube-proxy INSTANCES
each packet destined for the service IP/port pair is
intercepted and its destination address modified, so the packet is redirected to one of
the pods backing the service

Running highly available clusters

One of the reasons for running apps inside Kubernetes is to keep them running with-
out interruption with no or limited manual intervention in case of infrastructure
failures.

RUNNING MULTIPLE INSTANCES TO REDUCE THE LIKELIHOOD OF DOWNTIME

USING LEADER-ELECTION FOR NON-HORIZONTALLY SCALABLE APPS

The mechanism doesn’t need to be incorporated into the app itself. You can use a
sidecar container that performs all the leader-election operations and signals the
main container when it should become active. You’ll find an example of leader elec-
tion in Kubernetes at https://github.com/kubernetes/contrib/tree/master/election.

RUNNING AN ETCD CLUSTER
is run it on an appropriate number of machines (three, five, or seven, as
explained earlier in the chapter) and make them aware of each other.

RUNNING MULTIPLE INSTANCES OF THE API SERVER
API server is (almost
completely) stateless
need to be fronted by a load balancer

ENSURING HIGH AVAILABILITY OF THE CONTROLLERS AND THE SCHEDULER

when running multiple instances of these components, only one
instance may be active at any given time.

they work with leader election

$ kubectl get endpoints kube-scheduler -n kube-system -o yaml



