# Advanced scheduling

16.1 Using taints and tolerations to repel pods from
certain nodes

node selectors and node affinity rules make
it possible to select which nodes a pod can or can’t be scheduled to


taints allow rejecting deployment of pods to certain nodes
Pods that you want deployed on a tainted node need to opt in to use
the node

DISPLAYING A NODE’S TAINTS

kubectl describe node
Taints: node-role.kubernetes.io/master:NoSchedule

Taints have a key, value, and an effect
This taint prevents pods from being scheduled to the master node, unless those pods
tolerate this taint.

DISPLAYING A POD’S TOLERATIONS

$ kubectl describe po kube-proxy-80wqm -n kube-system

Tolerations:
node-role.kubernetes.io/master=:NoSchedule
node.alpha.kubernetes.io/notReady=:Exists:NoExecute
node.alpha.kubernetes.io/unreachable=:Exists:NoExecute


possible effects exist

NoSchedule
won’t be scheduled

PreferNoSchedule
prefer not to  scheduled

NoExecute
pods that are already running on that node and don’t tolerate the
NoExecute taint will be evicted from the node.

16.1.2 Adding custom taints to a node
$ kubectl taint node node1.k8s node-type=production:NoSchedule

16.1.3 Adding tolerations to pods

Listing 16.4 A production Deployment with a toleration: production-deployment.yaml

16.1.4 Understanding what taints and tolerations can be used for

USING TAINTS AND TOLERATIONS DURING SCHEDULING

You can set up taints and tolerations any way you see fit. For example, you could
partition your cluster into multiple partitions, allowing your development teams to
schedule pods only to their respective nodes.

several of your nodes provide special hardware

CONFIGURING HOW LONG AFTER A NODE FAILURE A POD IS RESCHEDULED

Tolerations:
key: node.alpha.kubernetes.io/notReady
key: node.alpha.kubernetes.io/unreachable

16.2 Using node affinity to attract pods to certain nodes

failure-domain.beta.kubernetes.io/region specifies the geographical region
the node is located in.
failure-domain.beta.kubernetes.io/zone specifies the availability zone the
node is in.
kubernetes.io/hostname is obviously the node’s hostname.

Listing 16.9 A pod using a nodeAffinity rule: kubia-gpu-nodeaffinity.yaml

MAKING SENSE OF THE LONG NODEAFFINITY ATTRIBUTE NAME

affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:

requiredDuringScheduling... means the rules defined under this field spec-
ify the labels the node must have for the pod to be scheduled to the node.

...IgnoredDuringExecution means the rules defined under the field don’t
affect pods already executing on the node.

16.2.2 Prioritizing nodes when scheduling a pod

preferredDuringSchedulingIgnoredDuringExecution

 If your machines don’t have enough room for the pods or if other important reasons exist that prevent them from being scheduled there, you’re okay with them being scheduled to the machines your partners use and to the other zones.

LABELING NODES
$ kubectl label node node1.k8s availability-zone=zone1
$ kubectl label node node1.k8s share-type=dedicated

Listing 16.11 Deployment with preferred node affinity: preferred-deployment.yaml

UNDERSTANDING HOW NODE PREFERENCES WORK

the Scheduler also uses other prioritization functions to decide where to schedule a pod

SelectorSpreadPriority function, which makes sure pods belonging to the same ReplicaSet or Service are spread around different nodes so a node failure won’t bring the whole service down.

16.3 Co-locating pods with pod affinity and anti-affinity

For example, imagine having a frontend and a backend pod. Having those pods
deployed near to each other reduces latency and improves the performance

16.3.1 Using inter-pod affinity to deploy pods on the same node

First, deploy the backend pod:
$ kubectl run backend -l app=backend --image busybox -- sleep 999999

SPECIFYING POD AFFINITY IN A POD DEFINITION
Listing 16.13 Pod using podAffinity: frontend-podaffinity-host.yaml

UNDERSTANDING HOW THE SCHEDULER USES POD AFFINITY RULES

if you now delete the backend pod, the Scheduler will schedule the pod to node2 even though it doesn’t define any pod affinity rules itself
Scheduler takes other pods’ pod affinity rules into account,

Listing 16.15 Scheduler log showing why the backend pod is scheduled to node2
... backend-qhqj6 -> node2.k8s: Taint Toleration Priority, Score: (10)

CO-LOCATING PODS IN THE SAME AVAILABILITY ZONE
topologyKey property to failure-domain.beta.kubernetes.io/zone.

CO-LOCATING PODS IN THE SAME GEOGRAPHICAL REGION
topologyKey would be set to failure-domain.beta.kubernetes.io/region.

16.3.3 Expressing pod affinity preferences instead of hard requirements

Listing 16.16 Pod affinity preference
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 80
podAffinityTerm:
topologyKey: kubernetes.io/hostname
labelSelector:
matchLabels:
app: backend

16.3.4 Scheduling pods away from each other with pod anti-affinity
example of why you’d want to use pod anti-affinity is when two sets of pods inter-
fere with each other’s performance if they run on the same node

spread pods of the same group across different availability zones or regions,

USING ANTI-AFFINITY TO SPREAD APART PODS OF THE SAME DEPLOYMENT

Listing 16.18 Pods with anti-affinity: frontend-podantiaffinity-host.yaml

USING PREFERENTIAL POD ANTI-AFFINITY
preferredDuringSchedulingIgnoredDuringExecution

