# Managing pods’ computational resources

14.1 Requesting resources for a pod’s containers

requests-pod.yaml

use at most 10 mebibytes of RAM
requests don’t limit the amount of CPU a container can use.

14.1.2 Understanding how resource requests affect scheduling

minimum amount of resources your pod need

Scheduler will only consider nodes with enough
unallocated resources

Scheduler  look at sum of resources requested by the existing pods deployed on the node.

two prioritization functions rank
nodes based on the amount of resources requested: LeastRequestedPriority and
MostRequestedPriority

$ kubectl describe nodes

$ kubectl run requests-pod-2 --image=busybox --restart Never --requests='cpu=800m,memory=20Mi' -- dd if=/dev/zero of=/dev/null

POD THAT DOESN’T FIT ON ANY NODE

kubectl run requests-pod-3 --image=busybox --restart Never
 --requests='cpu=1,memory=20Mi' -- dd if=/dev/zero of=/dev/null

kubectl get po requests-pod-3
stuck at Pending

14.1.3 Understanding how CPU requests affect CPU time sharing

if one container wants to use up as much CPU as it can will be allowed,
As soon as the second container needs CPU time, it will get it and the first container will be throttled back

14.1.4 Defining and requesting custom resources

An example of a custom resource could be the number of GPU units available on the
node. Pods requiring the use of a GPU specify that in their requests. The Scheduler then makes sure the pod is only scheduled to nodes with at least one GPU still unallocated.

14.2 Limiting resources available to a container

limited-pod.yaml
The process or processes running inside the container will not be allowed to consume
more than 1 CPU core and 20 mebibytes of memory

resource requests will be set to the same values as the resource limits

14.2.2 Exceeding the limits
if it keeps going over the memory limit and getting killed, Kubernetes will begin restarting it with increasing delays between restarts.

CrashLoopBackOff
20, 40, 80, and 160 seconds, and finally limited to 300 seconds

kubectl describe pod
OOMKilled
containers can get OOMKilled even if they aren’t over their limit.

UNDERSTANDING THAT CONTAINERS ALWAYS SEE THE NODE’S MEMORY, NOT THE CONTAINER’S

The problem is visible when running Java apps, especially if you don’t specify the
maximum heap size for the Java Virtual Machine with the -Xmx option

JVM will set the maximum heap size based on the host’s total memory instead of
the memory available to the container

And if you think setting the -Xmx option properly solves the issue, you’re wrong,
unfortunately. The -Xmx option only constrains the heap size, but does nothing about
the JVM’s off-heap memory. Luckily, new versions of Java alleviate that problem by tak-
ing the configured container limits into account.

UNDERSTANDING THAT CONTAINERS ALSO SEE ALL THE NODE’S CPU CORES

Certain applications look up the number of CPUs on the system to decide how
many worker threads they should run
You may want to use the Downward API to pass the CPU limit to the container and
use it instead of relying on the number of CPUs your app can see on the system
/sys/fs/cgroup/cpu/cpu.cfs_quota_us
/sys/fs/cgroup/cpu/cpu.cfs_period_us

14.3 Understanding pod QoS classes

Quality of Service (QoS) classes:
 BestEffort (the lowest priority)
 Burstable
 Guaranteed (the highest)

The lowest priority QoS class is the BestEffort class.

In the worst case, they may get almost no CPU time at all and will be the first ones killed when memory needs to be freed for other pods.


Guaranteed QoS class. This class is given to pods whose containers’ requests are equal to the limits for all resources.
Requests and limits need to be set for both CPU and memory.
They need to be set for each container.
Request = limit


Burstable pods get the amount of resources they request, but are allowed to use addi-
tional resources (up to the limit) if needed.

Pod’s QoS class
BestEffort or Guaranteed only when all containers are in same QoS.

14.3.2 Understanding which process gets killed when memory is low
BestEffort class, followed by Burstable pods, and finally Guaranteed

UNDERSTANDING HOW CONTAINERS WITH THE SAME QOS CLASS ARE HANDLED
Containers of the same classes are killed by memory usage.

14.4 Setting default requests and limits for pods per
namespace

LimitRange resource

used by the LimitRanger Admission Control plugin
prevent users from creating pods that are bigger than any node in the cluster

apply to each individual pod

They don’t limit the total amount of resources available across all the pods in
the namespace.

A LimitRange resource: limits.yaml

14.5 Limiting the total resources available in a namespace
ResourceQuota object

RESOURCEQUOTA FOR CPU AND MEMORY
A ResourceQuota resource for CPU and memory: quota-cpu-memory.yaml
applies to all the pods’ resource requests and limits in total and not to each individual pod or container separately

ResourceQuota with kubectl describe quota

When a quota for a specific resource (CPU or memory) is configured (request or
limit), pods need to have the request or limit (respectively) set for that same resource;

14.5.2 Specifying a quota for persistent storage
A ResourceQuota for storage: quota-storage.yaml

14.5.3 Limiting the number of objects that can be created
number of Pods, ReplicationControllers, Services

A ResourceQuota for max number of resources: quota-object-count.yaml

14.5.4 Specifying quotas for specific pod states and/or QoS classes
Four scopes are currently available: BestEffort, NotBestEffort, Terminating, and NotTerminating.

Listing 14.17 ResourceQuota for BestEffort/NotTerminating pods:
quota-scoped.yaml

14.6 Monitoring pod resource usage

14.6.1 Collecting and retrieving actual resource usages
cAdvisor,
Heapster.

ENABLING HEAPSTER
$ minikube addons enable heapster
$ kubectl top node
$ kubectl top pod --all-namespaces

Heapster, which aggregates the data over a few minutes and doesn’t expose it immediately.

14.6.2 Storing and analyzing historical resource consumption statistics
InfluxDB for storing statistics data and Grafana for visualizing and analyzing them

ANALYZING RESOURCE USAGE WITH GRAFANA

$ kubectl cluster-info
$ minikube service monitoring-grafana -n kube-system

USING THE INFORMATION SHOWN IN THE CHARTS

By looking at the charts, you can quickly see if the resource requests or limits you’ve set for your pods need to be raised or whether they can be lowered to allow more pods to fit on your nodes.

