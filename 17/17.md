# Extending Kubernetes

18.1 Defining custom API objects

18.1.1 Introducing CustomResourceDefinitions

INTRODUCING THE EXAMPLE CUSTOMRESOURCEDEFINITION

Listing 18.1 An imaginary custom resource: imaginary-kubia-website.yaml
Before you can create instances of your custom object, you need to make Kubernetes
recognize them.

Listing 18.2 A CustomResourceDefinition manifest: website-crd.yaml

$ kubectl create -f website-crd-definition.yaml

$ kubectl get website kubia -o yaml

CREATING AN INSTANCE OF A CUSTOM RESOURCE

Listing 18.3 A custom Website resource: kubia-website.yaml
$ kubectl create -f kubia-website.yaml


RETRIEVING INSTANCES OF A CUSTOM RESOURCE
$ kubectl get websites
$ kubectl get website kubia -o yaml

DELETING AN INSTANCE OF A CUSTOM OBJECT
$ kubectl delete website kubia

In general, the point of creating custom objects like this isn’t always to make some-
thing happen when the object is created. Certain custom objects are used to store data
instead of using a more generic mechanism such as a ConfigMap. Applications run-
ning inside pods can query the API server for those objects and read whatever is
stored in them.

18.1.2 Automating custom resources with custom controllers

https://github.com/luksa/k8s-website-controller.

UNDERSTANDING WHAT THE WEBSITE CONTROLLER DOES

Immediately upon startup, the controller starts to watch Website objects by requesting
the following URL:
http://localhost:8001/apis/extensions.example.com/v1/websites?watch=true

The API server sends the ADDED watch event every time a new Website object is cre-
ated.

extracts the Website’s name and
the URL of the Git repository from the Website object it received in the watch event
and creates a Deployment and a Service object by posting their JSON manifests to the
API server.

Deployment resource contains a template for a pod with two containers
(shown in figure 18.4): one running an nginx server and another one running a git-
sync process

The API server also sends a DELETED watch event when a Website resource instance is
deleted.

Upon receiving the event, the controller deletes the Deployment and the Ser-
vice resources it created earlier.


The proper way to watch objects through the API server is to not only
watch them, but also periodically re-list all objects in case any watch events
were missed.


RUNNING THE CONTROLLER AS A POD
Listing 18.5 A Website controller Deployment: website-controller.yaml


If Role Based Access Control (RBAC) is enabled in your cluster,
ou’ll need to bind the website-controller ServiceAccount to the
cluster-admin ClusterRole,

kubectl create clusterrolebinding website-controller --clusterrole=cluster-admin --serviceaccount=default:website-controller


SEEING THE CONTROLLER IN ACTION

$ kubectl create -f kubia-website.yaml

18.1.3 Validating custom objects

The API server doesn’t validate the contents of the YAML (except
the usual fields like apiVersion, kind, and metadata)

CustomResourceValidation
only available in Kubernetes 1.8 and later

18.1.4 Providing a custom API server for your custom objects

implement your own API server and have the clients talk directly to it

INTRODUCING API SERVER AGGREGATION

you could create an API server responsible for handling your Website
objects. It could validate those objects the way the core Kubernetes API server validates them.

REGISTERING A CUSTOM API SERVER

Listing 18.8 An APIService YAML definition

CREATING CUSTOM CLIENTS

ou can also build a custom CLI tool. This will allow you to add
dedicated commands for manipulating those objects, similar to how kubectl

18.2 Extending Kubernetes with the Kubernetes Service
Catalog

“Hey, I need a PostgreSQL data-
base. Please provision one and tell me where and how I can connect to it.” This will
soon be possible through the Kubernetes Service Catalog.

18.2.1 Introducing the Service Catalog

Service Catalog is a catalog of services. Users can browse
through the catalog and provision instances of the services listed in the catalog by
themselves without having to deal with Pods, Services, ConfigMaps, and other resources
required for the service to run.


18.2.2 Introducing the Service Catalog API server and Controller
Manager

Service Catalog API Server
etcd as the storage
Controller Manager, where all the controllers run

18.2.3 Introducing Service Brokers and the OpenServiceBroker API

You’ll find the OpenServiceBroker API spec at https://github.com/openservicebro kerapi/servicebroker.

REGISTERING BROKERS IN THE SERVICE CATALOG

Listing 18.9 A ClusterServiceBroker manifest: database-broker.yaml

LISTING THE AVAILABLE SERVICES IN A CLUSTER
$ kubectl get clusterserviceclasses
$ kubectl get serviceclass postgres-database -o yaml

18.2.4 Provisioning and using a service

PROVISIONING A SERVICEINSTANCE

Listing 18.12 A ServiceInstance manifest: database-instance.yaml
$ kubectl get instance my-postgres-db -o yaml

BINDING A SERVICEINSTANCE
Listing 18.14 A ServiceBinding: my-postgres-db-binding.yaml

USING THE NEWLY CREATED SECRET IN CLIENT PODS

18.2.5 Unbinding and deprovisioning

$ kubectl delete servicebinding my-postgres-db-binding

18.2.6 Understanding what the Service Catalog brings

In general, service brokers allow easy provisioning and exposing of services in
Kubernetes and will make Kubernetes an even more awesome platform for deploying
your applications.

18.3 Platforms built on top of Kubernetes

18.3.1 Red Hat OpenShift Container Platform

INTRODUCING ADDITIONAL RESOURCES AVAILABLE IN OPENSHIFT
OpenShift provides some additional API objects in addition to all those available in
Kubernetes.

The additional resources include
 Users & Groups
 Projects
 Templates
 BuildConfigs
 DeploymentConfigs
 ImageStreams
 Routes
 And others

UNDERSTANDING USERS, GROUPS, AND PROJECTS
 OpenShift provides a proper multi-tenant environment to its users

specify what each user can do and what they cannot. These features pre-date the Role-
Based Access Control, which is now the standard in vanilla Kubernetes.

INTRODUCING APPLICATION TEMPLATES

it’s a list of objects whose definitions can include placeholders that get replaced with parameter values when you process and then instantiate a template


BUILDING IMAGES FROM SOURCE USING BUILDCONFIGS
build and immediately deploy an application in the OpenShift cluster by pointing it to a Git repository holding the application’s source code.


AUTOMATICALLY DEPLOYING NEWLY BUILT IMAGES WITH DEPLOYMENTCONFIGS

DeploymentConfig is almost identical to the Deployment object in Kubernetes, but
it pre-dates it. Like a Deployment object, it has a configurable strategy for transitioning between Deployments. 

EXPOSING SERVICES EXTERNALLY USING ROUTES

Similar to an Ingress controller, a Route needs a Router, which is a controller that
provides the load balancer or proxy.

TRYING OUT OPENSHIFT

If you’re interested in trying out OpenShift, you can start by using Minishift,

18.3.2 Deis Workflow and Helm

Helm, which is gaining traction in the Kubernetes
community as a standard way of deploying existing apps in Kubernetes. We’ll take a
brief look at both.

Deploying new versions of your app is triggered by pushing your changes with git
push deis master and letting Workflow take care of the rest. Similar to OpenShift,
Workflow also provides a source to image mechanism, application rollouts and roll-
backs, edge routing, and also log aggregation, metrics, and alerting, which aren’t
available in core Kubernetes.

DEPLOYING RESOURCES THROUGH HELM

Helm is a package manager for Kubernetes (similar to OS package managers like yum
or apt in Linux or homebrew in MacOS).

A helm CLI tool (the client).
Tiller, a server component running as a Pod inside the Kubernetes cluster.

Helm application packages are called Charts.
https://github.com/kubernetes/charts.

Once someone prepares a Helm chart for a specific application and adds it to the
Helm chart GitHub repo, installing the whole application takes a single one-line com-
mand.



