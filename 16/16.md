# Best practices for developing apps

17.2 Understanding the pod’s lifecycle

 apps running in a pod can be killed any time

17.2.1 Applications must expect to be killed and relocated

EXPECTING THE LOCAL IP AND HOSTNAME TO CHANGE

The pod’s IP will change nevertheless. Apps need to be prepared for that
to happen.
if basing it on the hostname, should always use a StatefulSet

EXPECTING THE DATA WRITTEN TO DISK TO DISAPPEAR

Even during the lifetime of a single pod, the files written to disk by
the app running in the pod may disappear.

USING VOLUMES TO PRESERVE DATA ACROSS CONTAINER RESTARTS
To make sure data like this isn’t lost, you need to use at least a pod-scoped volume

Using volumes to preserve files across con-
tainer restarts like this is a double-edged sword. You need to think carefully about
whether to use them or not.

17.2.2 Rescheduling of dead or partially dead pods

The ReplicaSet controller doesn’t care if the pods are dead
cares about is that the number of pods matches the desired replica count

17.2.3 Starting pods in a specific order
 you can prevent a pod’s main container from starting until a precondition is
met. This is done by including an init containers in the pod.

ADDING AN INIT CONTAINER TO A POD

Listing 17.2 An init container defined in a pod: fortune-client.yaml

BEST PRACTICES FOR HANDLING INTER-POD DEPENDENCIES

but it’s much better to write apps that don’t require every service
they rely on to be ready before the app starts up.

it should signal that through its readiness probe, so
Kubernetes knows it, too, isn’t ready.

17.2.4 Adding lifecycle hooks

USING A POST-START CONTAINER LIFECYCLE HOOK

Post-start hooks allow you to run additional commands without having to touch
the app.

Until the hook completes, the container will stay in the Waiting state with the
reason ContainerCreating.

Listing 17.4 Pod’s events showing the exit code of the failed command-based hook

The standard and error outputs of command-based post-start hooks aren’t logged any-
where, so you may want to have the process the hook invokes log to a file in the con-
tainer’s filesystem,

USING A PRE-STOP CONTAINER LIFECYCLE HOOK
can be used to initiate a graceful shutdown of the container

A pre-stop hook YAML snippet: pre-stop-hook-httpget.yaml

USING A PRE-STOP HOOK BECAUSE YOUR APP DOESN’T RECEIVE THE SIGTERM SIGNAL
mistake of defining a pre-stop hook solely to send a SIGTERM

If your container image is configured to run a shell, which in turn runs the app process, the signal may be eaten up by the shell itself, instead of being passed down to the child process.

UNDERSTANDING THAT LIFECYCLE HOOKS TARGET CONTAINERS, NOT PODS
This may happen multiple times in the pod’s life-
time,

17.2.5 Understanding pod shutdown

deletion of the Pod object through the API server
deletionTimestamp
Kubelet notices the pod needs to be terminated
starts terminating each of the pod’s containers.


SPECIFYING THE TERMINATION GRACE PERIOD
spec.terminationGracePeriodSeconds field. It defaults to 30,


$ kubectl delete po mypod --grace-period=5
$ kubectl delete po mypod --grace-period=0 --force

By force-deleting a pod, you’ll cause the controller to create a
replacement pod without waiting for the containers of the deleted pod to shut
down

Only delete stateful pods forcibly when you’re absolutely sure the pod isn’t running anymore or can’t talk to the other members of the cluster

IMPLEMENTING THE PROPER SHUTDOWN HANDLER IN YOUR APPLICATION
starting their shut-down procedure and terminating when it finishes
app then only has a fixed amount of time to terminate cleanly

REPLACING CRITICAL SHUT-DOWN PROCEDURES WITH DEDICATED SHUT-DOWN PROCEDURE PODS

to handle this problem is by having a dedicated, constantly run-
ning pod that keeps checking for the existence of orphaned data. When this pod finds
the orphaned data, it can migrate it to the remaining pods. Rather than a constantly
running pod, you can also use a CronJob resource and run the pod periodically.

17.3 Ensuring all client requests are handled properly

17.3.1 Preventing broken client connections when a pod is starting up
pod also needs to signal to Kubernetes that it’s ready

If your app isn’t ready to accept connections by then, clients will see “connec-
tion refused” types of errors.

All you need to do is make sure that your readiness probe returns success only
when your app is ready to properly handle incoming requests. A good first step is to
add an HTTP GET readiness probe and point it to the base URL of your app.

17.3.2 Preventing broken connections during pod shut-down

UNDERSTANDING THE SEQUENCE OF EVENTS OCCURRING AT POD DELETION
sequence A
run the pre-stop hook, send SIGTERM, wait for a period of time, and
then forcibly kill the container if it hasn’t yet terminated on its own

If the app
responds to the SIGTERM by immediately ceasing to receive client requests, any client
trying to connect to it will receive a Connection Refused error.

sequence B
Most likely, the time it takes to shut down the app’s process in the pod is slightly shorter than the time required for the iptables rules to be updated.

A high probability exists that the SIGTERM signal will be sent well
before the iptables rules are updated on all nodes.

If the app closes the server socket and stops accepting connections
immediately, this will cause clients to receive “Connection Refused”

SOLVING THE PROBLEM

adding a readiness probe to your pod will solve the problem.
make the readiness probe start failing as soon as the pod receives the SIGTERM.

The only reasonable thing you can do is wait for a long-enough time to ensure all
the proxies have done their job.

A few seconds should be enough in most situations

It’s important to understand that you can’t solve
the problem perfectly, but even adding a 5- or 10-second delay should improve the
user experience considerably.

You can use a longer delay, but don’t go overboard,
will cause the pod to be shown in lists long after it has been deleted, which is always frustrating to the user deleting the pod.

WRAPPING UP THIS SECTION
Not as simple as exiting the process immediately upon receiving the termination sig nal 
But the least you can do is add a pre-stop hook that waits a few seconds,

Listing 17.7 A pre-stop hook for preventing broken connections
lifecycle:
preStop:
exec:
command:
- sh
- -c
- "sleep 5"

17.4 Making your apps easy to run and manage in Kubernetes

17.4.1 Making manageable container images

Smaller container images are easier to run and manage.
Use the FROM scratch directive in the Dockerfile for these images.

17.4.2 Properly tagging your images and using imagePullPolicy wisely

referring to the latest image tag in your pod manifests will cause problems

It’s almost mandatory to use tags containing a proper version designator instead
of latest, except maybe in development.

If the image pull policy is set to Always, the container runtime will contact the image registry every time a new pod is deployed.

this policy prevents the pod from starting up when the registry cannot be contacted.

17.4.3 Using multi-dimensional instead of single-dimensional labels

Make sure you add multiple labels to each resource, so they can be selected across each individual dimension.

The name of the application (or perhaps microservice) the resource belongs to
 Application tier (front-end, back-end, and so on)
 Environment (development, QA, staging, production, and so on)
 Version
 Type of release (stable, canary, green or blue for green/blue deployments, and
o on)
 Tenant (if you’re running separate pods for each tenant instead of using name-
paces)
 Shard for sharded systems

17.4.4 Describing each resource through annotations

At the least, resources should contain an annotation describing the resource and an annotation with contact information of the person responsible for it

Both labels and annotations make managing running applications much easier

17.4.5 Providing information on why the process terminated

Be nice to the ops people and make their lives easier by including all the necessary debug information in your log files.

/dev/termination-log,
but it can be changed by setting the terminationMessagePath field in the container
definition in the pod spec.

Listing 17.8 Pod writing a termination message: termination-message.yaml

It can also be used in pods that run a
completable task and terminate successfully

If the container doesn’t write the message to any file, you can set the
terminationMessagePolicy field to FallbackToLogsOnError.

17.4.6 Handling application logs

apps should write to the standard output instead of files.

use the --previous option with kubectl logs

$ kubectl exec <pod> cat <logfile>

COPYING LOG AND OTHER FILES TO AND FROM A CONTAINER
$ kubectl cp foo-pod:/var/log/foo.log foo.log

$ kubectl cp localfile foo-pod:/etc/remotefile

USING CENTRALIZED LOGGING
ELK stack composed of ElasticSearch, Logstash, and Kibana. A slightly modified variation is the EFK stack, where Logstash is replaced with FluentD

HANDLING MULTI-LINE LOG STATEMENTS

The solution may be to keep outputting human-readable logs to standard output,
while writing JSON logs to a file and having them processed by FluentD. This requires
configuring the node-level FluentD agent appropriately or adding a logging sidecar
container to every pod.

17.5 Best practices for development and testing

You can always develop and run apps on your local machine, the way you’re used
to. After all, an app running in Kubernetes is a regular (although isolated) process
running on one of the cluster nodes.

CONNECTING TO BACKEND SERVICES

Service and uses the BACKEND_SERVICE _HOST and BACKEND_SERVICE_PORT environment variables to find the Service’s coordinates,

CONNECTING TO THE API SERVER

If it uses the ServiceAccount’s token to authenticate itself, you
can always copy the ServiceAccount’s Secret’s files to your local machine with kubectl
cp

In this case, you’ll need to make sure the user account your local kubectl is using
has the same privileges as the ServiceAccount the app will run under.

RUNNING INSIDE A CONTAINER EVEN DURING DEVELOPMENT
you can always mount your local filesystem into the container through Docker volumes,

17.5.2 Using Minikube in development
nothing forces you to run your app inside Kubernetes

MOUNTING LOCAL FILES INTO THE MINIKUBE VM AND THEN INTO YOUR CONTAINERS

minikube mount command and then mount it into your containers
through a hostPath

USING THE DOCKER DAEMON INSIDE THE MINIKUBE VM TO BUILD YOUR IMAGES
you can use the Docker daemon inside the Minikube VM to do the building

$ eval $(minikube docker-env)

BUILDING IMAGES LOCALLY AND COPYING THEM OVER TO THE MINIKUBE VM DIRECTLY
$ docker save <image> | (eval $(minikube docker-env) && docker load)

COMBINING MINIKUBE WITH A PROPER KUBERNETES CLUSTER

17.5.3 Versioning and auto-deploying resource manifests

All you need to do is tell Kubernetes your desired state and it will take all the necessary actions to reconcile the cluster state with the desired state.

kubectl apply
https://github.com/box/kube-applier.

17.5.4 Introducing Ksonnet as an alternative to writing YAML/JSON manifests

17.5.5 Employing Continuous Integration and Continuous Delivery (CI/CD)



